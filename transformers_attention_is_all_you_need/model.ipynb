{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb3f6c4",
   "metadata": {},
   "source": [
    "# Attention is all you need\n",
    "This notebook breaks down and implements the seminal paper \"Attention is all you need\". I felt like a lot of existing examples left the actual implementation details (e.g. dimensions of each input/output and the actual dataflow) a bit unclear so I decided to code my own. A cleaner version of the code is available in transformer.py. For the paper, focus on section 3 for implementing the net and section 5 for training it.\n",
    "\n",
    "## Model Breakdown\n",
    "\n",
    "## Implementation Notes\n",
    "- p8, residual dropout. We apply dropout [ 33] to the output of each sub-layer, before it is added to the\n",
    "sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\n",
    "positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\n",
    "Pdrop = 0.1.\n",
    "- p5 \" In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation... in the embedding layers, we multiply those weights by âˆšdmodel\"\n",
    "\n",
    "## Parameters\n",
    "**General Model Parameters**\n",
    "- Input vocabulary composed of $n$ words: $x \\in \\mathbb{R}^{n}$\n",
    "- Output vocabulary composed of $m$ words: $y \\in \\mathbb{R}^{m}$\n",
    "- $d_{model}=512$ TODO idk what this means exactly\n",
    "  \n",
    "**Attention**\n",
    "- $h$ is n_heads in the code, i.e. the # of heads in multihead attention\n",
    "- $ d_k = d_{model} / h $\n",
    "- $h$ must be chosen to be a factor of $d_{model}$\n",
    "- $ d_v = d_k $\n",
    "- $ W_i^Q \\in \\mathbb{R}^{d_\\text{model} \\times d_k }, i=\\{1,...,h\\} $\n",
    "- $ W_i^K \\in \\mathbb{R}^{d_\\text{model} \\times d_k }, i=\\{1,...,h\\}  $\n",
    "- $ W_i^V \\in \\mathbb{R}^{d_\\text{model} \\times d_v }, i=\\{1,...,h\\}  $\n",
    "- $ W^O \\in \\mathbb{R}^{h d_v \\times d_\\text{model} }, i=\\{1,...,h\\}  $\n",
    "- Query vector: $Q \\in \\mathbb{R}^{1 \\times d_\\text{model}}$ (I assume)\n",
    "- Key vector: $K \\in \\mathbb{R}^{1 \\times d_\\text{model}}$ (I assume)\n",
    "- value vector: $V \\in \\mathbb{R}^{1 \\times d_\\text{model}}$ (I assume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383d8a98-378a-48eb-9696-be4e041bbd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4842ee",
   "metadata": {},
   "source": [
    "## Attention\n",
    "$ \\text{{Attention}}(Q, K, V) = \\text{{softmax}}\\left(\\frac{{QK^T}}{{\\sqrt{d_k}}}\\right)V $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5d582b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    mask: torch.Tensor = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Implements section 3.2.1\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d_k : int\n",
    "        d_model // # heads\n",
    "    query : torch.Tensor\n",
    "        shape (batch_size, h, _, d_k). _ is some value on [1,n] where n = size of input vocab\n",
    "    key : torch.Tensor\n",
    "        shape (batch_size, h, n, d_k)\n",
    "    value : torch.Tensor\n",
    "        shape (batch_size, h, n, d_v) (remember d_k == d_v)\n",
    "    mask : torch.Tensor, optional\n",
    "        shape (batch_size, 1, 1, n)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        shape (batch_size, h, _, d_v), same as query (also recall d_k == d_v)\n",
    "\n",
    "    TODO\n",
    "    -----\n",
    "    - I'm not sure if the size n for the dimensions is correct\n",
    "    - unittest against existing implementations\n",
    "    \"\"\"\n",
    "    d_k = query.shape[-1]\n",
    "    # QK^T from the paper is of dimensions 1,d_k x d_k = 1\n",
    "    # MatMul for dim > 2 matches the first dim-2 dimensions and matrix multiplies the last 2\n",
    "    # meaning to get query and key to match we need to do key.transpose(-2, -1)\n",
    "    # query: (batch_size, h, _, d_k), key: (batch_size, h, n, d_k), key.transpose(-2, -1): (batch_size, h, d_k, n)\n",
    "    # x: (batch_size, h, _, n)\n",
    "    x = query @ key.transpose(-2, -1) / sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        x = x.masked_fill(mask == 0, -1e9)\n",
    "    # dim=-1 bc the last dim corresponds to the # of input vocab words for our elements xi\n",
    "    # and we want to run the softmax sum over that dimension\n",
    "    return F.softmax(x, dim=-1) @ value\n",
    "\n",
    "\n",
    "# testing functionality\n",
    "d_model = 512\n",
    "batch_size = 1\n",
    "n = 10\n",
    "h = 8\n",
    "d_k = d_model // h\n",
    "d_v = d_k\n",
    "d_q = 5\n",
    "q = torch.tensor(np.random.random((batch_size, h, d_q, d_k)))\n",
    "k = torch.tensor(np.random.random((batch_size, h, n, d_k)))\n",
    "v = torch.tensor(np.random.random((batch_size, h, n, d_v)))\n",
    "mask = torch.tensor(np.random.random((batch_size, 1, 1, n)))\n",
    "assert scaled_dot_product_attention(q, k, v, mask).shape == (batch_size, h, d_q, d_v)\n",
    "\n",
    "q = torch.tensor(np.random.random((batch_size, d_q, d_k)))\n",
    "k = torch.tensor(np.random.random((batch_size, n, d_k)))\n",
    "v = torch.tensor(np.random.random((batch_size, n, d_v)))\n",
    "mask = torch.tensor(np.random.random((batch_size, 1, n)))\n",
    "assert scaled_dot_product_attention(q, k, v, mask).shape == (batch_size, d_q, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9f35a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Implements section 3.2.2\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert d_model % h == 0, \"h must be a factor of d_model\"\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.d_k = d_model // h\n",
    "        self.d_v = d_k\n",
    "        \n",
    "        # W matrices from section 3.2.2\n",
    "        # self.w_q = nn.Linear(d_model, d_k * h, bias=False)\n",
    "        # self.w_k = nn.Linear(d_model, d_k * h, bias=False)\n",
    "        # self.w_v = nn.Linear(d_model, d_v * h, bias=False)\n",
    "        # self.w_o = nn.Linear(h * d_v, d_model, bias=False)\n",
    "\n",
    "        # self.layers = nn.ModuleList(\n",
    "        #     [copy.deepcopy(layer) for _ in range(N_ENCODER_LAYERS)]\n",
    "        # )\n",
    "        self.w_q = nn.ModuleList([nn.Linear(d_model, d_k, bias=False) for _ in range(h)])\n",
    "        self.w_k = nn.ModuleList([nn.Linear(d_model, d_k, bias=False) for _ in range(h)])\n",
    "        self.w_v = nn.ModuleList([nn.Linear(d_model, d_v, bias=False) for _ in range(h)])\n",
    "        self.w_o = nn.Linear(h * d_v, d_model, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        query : torch.Tensor\n",
    "            shape (batch_size, _, d_model) (_ <= n)\n",
    "        key : torch.Tensor\n",
    "            shape (batch_size, n, d_model)\n",
    "        value : torch.Tensor\n",
    "            shape (batch_size, n, d_model)\n",
    "        mask : torch.Tensor, optional\n",
    "            shape (batch_size, 1, n)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            shape (batch_size, _, d_model)\n",
    "\n",
    "        TODO\n",
    "        ----\n",
    "        - I'm not sure if size n for the dimensions is correct\n",
    "        - unittest against existing implementations\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, :]\n",
    "        q = torch.stack([self.w_q[i](query) for i in range(self.h)], dim=1)\n",
    "        k = torch.stack([self.w_k[i](key) for i in range(self.h)], dim=1)\n",
    "        v = torch.stack([self.w_v[i](value) for i in range(self.h)], dim=1)\n",
    "        x = scaled_dot_product_attention(q, k, v, mask)\n",
    "        # x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_v)\n",
    "        return self.w_o(torch.cat([x[:,i] for i in range(self.h)], dim=-1))\n",
    "\n",
    "\n",
    "# testing functionality\n",
    "d_model = 512\n",
    "batch_size = 1\n",
    "n = 10\n",
    "h = 8\n",
    "n_q = 5\n",
    "q = torch.tensor(np.random.random((batch_size, n_q, d_model)), dtype=torch.float32)\n",
    "k = torch.tensor(np.random.random((batch_size, n, d_model)), dtype=torch.float32)\n",
    "v = torch.tensor(np.random.random((batch_size, n, d_model)), dtype=torch.float32)\n",
    "mask = torch.tensor(np.random.random((batch_size, 1, n)), dtype=torch.float32)\n",
    "# train network for 1 iteration\n",
    "mha = MultiHeadAttention(d_model, h)\n",
    "_ = mha(q, k, v, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e2316",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Networks\n",
    "Section 3.3  \n",
    "$\\text{FFN}(x) = \\text{max}(0,xW_1 + b_1)W_2 + b_2$  \n",
    "Input and output is of size $d_{\\text{model}}=512$ and the inner layer has dimensionality $d_{ff}=2048$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5d997f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"Implements section 3.3\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            shape (batch_size, _, d_model)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            shape (batch_size, _, d_model)\n",
    "\n",
    "        TODO\n",
    "        ----\n",
    "        - unittest against existing implementations\n",
    "        \"\"\"\n",
    "        return self.ff(x)\n",
    "    \n",
    "\n",
    "# testing functionality\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "batch_size = 1\n",
    "n = 10\n",
    "x = torch.tensor(np.random.random((batch_size, n, d_model)), dtype=torch.float32)\n",
    "ff = PositionwiseFeedForward(d_model, d_ff)\n",
    "assert ff(x).shape == x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74877bf",
   "metadata": {},
   "source": [
    "## Residual Connection\n",
    "Residual connection + layer normalization + dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7bbe09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \"\"\"Implements the residual connection along with layer normalization and dopout\n",
    "\n",
    "    pg. 3 \"we employ a residual connection...around each of the two sub-layers,\n",
    "    followed by layer normalization\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, p_dropout: float, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        d_model : int\n",
    "            size of the model\n",
    "        p_dropout : float\n",
    "            dropout probability in range [0, 1]\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, sublayer: nn.Module) -> torch.Tensor:\n",
    "        # x + dropout(sublayer(x)) implements the residual connection\n",
    "        return self.layer_norm(x + self.dropout(sublayer(x)))\n",
    "    \n",
    "\n",
    "# testing functionality\n",
    "d_model = 512\n",
    "p_dropout = 0.1\n",
    "batch_size = 1\n",
    "n = 10\n",
    "x = torch.tensor(np.random.random((batch_size, n, d_model)), dtype=torch.float32)\n",
    "sublayer = nn.Linear(d_model, d_model)\n",
    "rc = ResidualConnection(d_model, p_dropout)\n",
    "assert rc(x, sublayer).shape == x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d783286",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d38a6d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Encoder is composed of self-attention and feed forward\n",
    "\n",
    "    TODO\n",
    "    ----\n",
    "    - simplify the self attention stuff\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, self_attn: nn.Module, feed_forward: nn.Module, residual_connection: nn.Module, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.residual_connection = residual_connection\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.residual_connection(x, lambda x : self.self_attn(x, x, x, mask))\n",
    "        return self.residual_connection(x, self.feed_forward)\n",
    "    \n",
    "\n",
    "# testing functionality\n",
    "d_model = 512\n",
    "h = 8\n",
    "d_ff = 2048\n",
    "p_dropout = 0.1\n",
    "batch_size = 1\n",
    "n = 10\n",
    "x = torch.tensor(np.random.random((batch_size, n, d_model)), dtype=torch.float32)\n",
    "mask = torch.tensor(np.random.random((batch_size, 1, n)), dtype=torch.float32)\n",
    "self_attn = MultiHeadAttention(d_model, h)\n",
    "feed_forward = PositionwiseFeedForward(d_model, d_ff)\n",
    "residual_connection = ResidualConnection(d_model, p_dropout)\n",
    "encoder_layer = EncoderLayer(self_attn, feed_forward, residual_connection)\n",
    "assert encoder_layer(x, mask).shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c34c725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Transformer encoder module\n",
    "\n",
    "    pg 3, figure 1: this class implements the left half of the diagram\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer: nn.Module, n_encoder_layers: int, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [copy.deepcopy(layer) for _ in range(n_encoder_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for encoder\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# testing functionality\n",
    "d_model = 512\n",
    "h = 8\n",
    "d_ff = 2048\n",
    "p_dropout = 0.1\n",
    "n_encoder_layers = 6\n",
    "batch_size = 1\n",
    "n = 10\n",
    "x = torch.tensor(np.random.random((batch_size, n, d_model)), dtype=torch.float32)\n",
    "mask = torch.tensor(np.random.random((batch_size, 1, n)), dtype=torch.float32)\n",
    "self_attn = MultiHeadAttention(d_model, h)\n",
    "feed_forward = PositionwiseFeedForward(d_model, d_ff)\n",
    "\n",
    "encoder_layer = EncoderLayer(self_attn, feed_forward, ResidualConnection(d_model, p_dropout))\n",
    "encoder = Encoder(encoder_layer, n_encoder_layers)\n",
    "assert encoder(x, mask).shape == x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42090067",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2dc16b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSoftmax(nn.Module):\n",
    "    \"\"\"Linear + Softmax Layer to map the decoder output to the vocabulary\n",
    "\n",
    "    pg 3, figure 1: implements linear and softmax layers at the top right of the diagram\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, n_vocab: int, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        d_model : int\n",
    "            size of the model\n",
    "        n_vocab : int\n",
    "            size of the vocabulary\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.proj = nn.Linear(d_model, n_vocab)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.softmax(self.proj(x), dim=-1)\n",
    "    \n",
    "\n",
    "# testing functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab3fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b195c8c8",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d22a3232-c679-48e3-a576-5bb56f2edc6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters pulled from paper\n",
    "N_ENCODER_LAYERS = 6  # pg 3\n",
    "N_DECODER_LAYERS = 6  # pg 3\n",
    "D_MODEL = 512  # pg 3 TODO explain this better\n",
    "P_DROPOUT = 0.1  # pg 8\n",
    "N_HEADS = 8  # pg 5, number of paralell attention layers\n",
    "D_K = (\n",
    "    D_MODEL / N_HEADS\n",
    ")  # pg 5. dimension of key projection parameter matrix for multi-head attention\n",
    "D_V = D_K  # pg 5. dimension of value projection mat for mult-head attn\n",
    "D_FF = 2048  # pg. 5 dimension of feed forward network\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"Vanilla Encoder-Decoder NN Architecture\n",
    "\n",
    "    TODO\n",
    "    ----\n",
    "    - understand masking\n",
    "    - understand what src_embed and tgt_embed are\n",
    "    - add type hints for src_embed and tgt_embed\n",
    "    - rename variables with \"_\" if they should be private\n",
    "    - understand what the generator is and does\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: nn.Module,\n",
    "        decoder: nn.Module,\n",
    "        src_embed,\n",
    "        tgt_embed,\n",
    "        linear_softmax,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.linear_softmax = linear_softmax\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        tgt: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "        tgt_mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Performs forward pass i.e. computation at every call\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        src : torch.Tensor\n",
    "            source input\n",
    "        tgt : torch.Tensor\n",
    "            target input\n",
    "        src_mask : torch.Tensor\n",
    "            source mask\n",
    "        tgt_mask : torch.Tensor\n",
    "            target mask\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            output of forward pass\n",
    "        \"\"\"\n",
    "        return self.decoder(\n",
    "            self.tgt_embed(tgt),\n",
    "            self.encoder(self.src_embed(src), src_mask),\n",
    "            src_mask,\n",
    "            tgt_mask,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
